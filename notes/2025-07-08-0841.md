---
tags:
  - zettelkasten
  - note
  - data-plane-os
  - oxide
date: 2025-07-08
created: 2025-07-0808:41
---
# data plane OS: chat with Ryan Goodfellow|Ry

yesterday i had this conversation with Ryan Goodfellow:

> [!quote] Ryan Goodfellow:
> Regarding the convo we had in the office the other day. The [Versal 2 Premium](https://www.amd.com/en/products/adaptive-socs-and-fpgas/versal/gen2/premium-series.html) chip we're currently looking at is an SoC with two Arm processors on it.
>
> - Dual core Arm Cortex A72
> - Dual core Arm Cortex R5F
>
> Not sure what were going to do with those just yet, but thought you'd find this interesting and would love to chat about potential use cases, probably writing some parts of it up in [RFD 575](https://rfd.shared.oxide.computer/rfd/0575).

> [!quote] **me**:
> 
> oooh, interesting
> this is definitely relevant to my vaguely specced out interests…
> i guess the thing i would start with is just, what do you imagine would run on the FPGA and what would happen on ARM? i kind of assume the goal would be to push “as much of the data plane as possible” into the FPGA, and the arm cores would be doing mostly control plane stuff…but, are there data path things you would want to do on ARM as well?
> (i should read the RFD…)

> [!quote] Ryan Goodfellow:
> There could potentially be things that are in fact well suited to the Arm cores, either A series or the R series. I'm looking at it as a function placement problem and want to get the most out of the chip as possible. The programmable logic parts (PL) parts of the FPGA are definitely good at massively parallel stuff, and the DSP cores are essentially VLIW machines AFAICT so far, so those fit very nicely for certain types of network functions. But there are also likely things that suit (super)scalar processors as well, and we should take advantage of that and figure out how to optimally use the resources that are at our disposal.

> [!quote] me:
> i also wonder who ends up being responsible for “administrative”/system enablement tasks like actually bringing up the NoC and PHYs, loading the FPGA bitstream, and so on — whether that’s a responsibility of the ARM cores on the part or if it’s something we can have the host system do over PCIe (and if so, how much the NPU needs to do to help out with that). i imagine putting the host OS in the driver’s seat there is more in keeping with Bryan Cantrill Thought, but depending on how the Versal 2 actually works, there might need to be software on there that enables it — depends on what is exposed over PCIe i guess

>[!quote] Ryan Goodfellow:
> In the older generations of the FPGAs that I've used it's been the host that does all that stuff. But it looks like on the newer generation the ARM cores are involved or driving the show entirely.

>[!quote] Ryan Goodfellow:
> The way I've been thinking about ==the overall dataplane is as a multi-stage pipeline, where individual stages can run on different parts of the chip==. Each pipeline stage can be thought of as a run-to-completion function, taking a set of packets as input and producing a different set of packets as output. ==Buffering queues interconnect the stages.== It's a very loose idea, but I've started to put together some example programs based on this model.

> [!quote] **me**:
> this makes sense with how i was kind of imagining the data plane OS i’ve been rotating in my mind:
>    
> - multitasking is purely cooperative, a workflow has exclusive ownership over a core unless it decides to yield
> - communication between cores/the workloads assigned to them is via shared memory queues, so you only yield to the OS if all your inbound queues are empty or your outbound queues are full
>    + any occasional system call would probably happen this way as well
> - in sum this seems like the most effective scheduling model for tasks that are mostly run-to-completion and where a core basically runs the same workflow over and over again in a loop; individual units of that workflow can be multitasked internally within the worker process if it wants to, because it has exclusive ownership over its core
> 
> this is inspired somewhat by seastar, io_uring on Linux, and Tokio.

> [!quote] Ryan Goodfellow:
> Awesome. I've also found ==trying to constrain total pipeline execution to a particular value, say 1 microsecond== useful. As that helps ground some of the decision making
> 
> It also helps to reason about execution behavior in the context of a larger system.

> [!quote] **me**:
>  interesting, so that actually means you could have something like preemptive multitasking. but i also think you don’t actually want preemptive multitasking because then you have a bunch of extra scheduler overhead. instead i think you want purely cooperative multitasking but you want to enforce that constraint on the workload so that the cooperative multitasking _behaves_ like preemption
>  
> (please bear in mind that these ideas are all mostly just coming from my daydreams and i cannot promise they will make sense in practice)

> [!quote] Ryan Goodfellow:
> So I think one of the ways to handle this is having a restricted execution model where the compiler can guarantee that the worst case path in the execution tree still satisfies the timing constraints

> [!quote] **me**:
> that whole be ideal if you’re able to pull off that kind of analysis (i assume it would imply restricting the control-flow constructs you accept to avoid having to solve the halting problem…)
> 
> another option is to have some kinda OS watchdog that kills any pipeline that overstays its time slice, but it would be nice not to have to do that, since the naive idea of doing it puts back the overhead of preemptive scheduling except without the “scheduling” part. it may be possible to design it in such a way that avoids that by having the pipeline disarm the timer when it finishes so you only have to jump to kernel space in the case where the pipeline _doesn’t_ do what it’s supposed to, but i’d have to spend some time with the datasheets before i can say if that’s possible…
