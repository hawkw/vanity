# data plane OS brain dump

**Date**: 2025-05-24

## core concept

exokernel for mostly homogenous tasks: packet processing, compute. ==the goal is to maximize throughput for basically one workload, where each work unit may not be totally identical.== for instance, packet processing flows might be UDP, TCP, ICMP, etc; may or may not require Geneve encap/decap; might involve other stuff like encryption offloads, but they basically all start and end by reading packets off one queue and putting them on another queue. plus some comparatively small amount of “back-office” tasks like the host reading counters or updating routing tables.

workload tasks have a separate user address space (so NOT a unikernel) to provide memory protection, live reloading of the workload program, debuggability; but tasks have direct access to hardware peripherals whenever possible (bring their own drivers).

the kernel provides a (cooperative?) scheduler, IRQ dispatching (if necessary), memory management (at the page level) + some ways to map MMIO directly into userspace tasks so they can have raw HW access. the idea is that a packet processing flow or compute task should be able to exclusively claim a core (or tenstorrent tensix core group thing) and run to completion with as few context switches as possible, unless it wants to yield to the scheduler.
## target architectures

- `aarch64` Neoverse (Xsight E1)
- `rv64` (Tenstorrent Blackhole)

## name ideas

both of these are unfortunately good, not sure which to pick. :(

- **vanity**: like "Hubris (girl version)”, secondary meaning is that it’s kind of like a vanity project for me personally
- **hekate**: in keeping with the theme that Oxide operating systems have names that begin with the letter “H” and are vaguely ancient Greek (Hubris, Helios). sexy witch goddess, which is appealing because i think this whole thing is very hot. the “eka” in “hekate” sounds sort of like the “exo” in “exokernel” if you squint. 
## notes

- investigate perf characteristics of permanently claiming one (or more) core(s) for the kernel and the rest for compute tasks, and communicating via message passing/atomic mailboxes instead of context switch. i don’t know whether this makes sense but it could be cool.
	- this may especially not make sense on tenstorrent. although maybe each tensix core could assign one of its “baby RISC-V” cores to run the kernel, but that probably results in wasted CPU time if we are able to spend as little time in the kernel as i’m hoping we can.
	- probably makes more sense on an E1-like device where just claiming one of 64 or 32 cores doesn’t “waste” as much of the data plane?
- talk to [[Ryan Goodfellow]] about what the packet workflows would actually be like

### how to deal with peripherals that are necessarily shared? 

okay so this whole idea of “give userspace direct MMIO mappings” falls down a little when you get to stuff that is inherently shared because there’s less of it then there are cores. for instance, E1 has 8 112GbE SerDes/two MACs, how are they shared between workers? 

do we go with a microkernel-y approach á la Hubris and give each MAC a userspace “server” process that “owns” it? now you have IPC overhead and you need to design a fast IPC system. do we put that stuff in kernel space? this is fewer context switches than an IPC, since it’s just user->kernel->user instead of  user1->kernel->user2->kernel->user1, but…this was supposed to be an exokernel, and the whole point was to avoid having to go to kernel space at all. or maybe we basically say there’s one big userspace process which has its own scheduler and owns multiple cores, so then the sharing of resources like MACs is just in-process synchronization. but then, why have a user/kernel boundary at all? if you only have one process, you can’t really hot-reload userspace; you have to kill the whole old user process in one go and start the new one. and you don’t have much fault isolation since any part of userspace crashing kills the whole thing. at that point, just be a unikernel!

so, here’s what i think makes the most sense: ==you “shard” userspace based on the hardware resource that submits and/or completes a workflow.== so, for the E1, you would have a separate user process that owns each MAC and say 16 PCIe lanes. that user process has some kind of userspace scheduler and and handles all the TX/RX activity on that MAC, so synchronization is mostly in process. you can update the user software on one MAC independently of the other, and a userspace crash should only take down one MAC, not both. activities that are infrequent or not latency sensitive can be farmed out to a separate process over some kinda IPC if desired, but don’t have to be.

i should look into whether Neoverse’s interrupt controller is sophisticated enough to give each MAC core affinity with a separate IRQ handler or something so they don’t contend over IRQ dispatching from the peripheral. more research into the architecture is needed.
